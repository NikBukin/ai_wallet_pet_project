from langchain_community.llms import YandexGPT
from langchain.agents import initialize_agent
from langchain.agents.agent_types import AgentType
from langchain_core.tools import tool
from langchain_core.callbacks import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager
from dotenv import load_dotenv
import os
import json
from models.llm_insert_active import llm_insert_active
from models.llm_news_analysis import llm_news_analysis, llm_find_active
from models.llm_sql_database_toolkit import llm_sql_database_toolkit
from models.llm_with_RAG import  llm_with_RAG

load_dotenv()
folder_id = os.getenv("FOLDER_ID_LLM")
api_key = os.getenv("API_KEY_LLM")
llm = YandexGPT(folder_id=folder_id, api_key=api_key)


class ToolUsageCallbackHandler(BaseCallbackHandler):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π –∫–æ–ª–±—ç–∫ —Ö–µ–Ω–¥–ª–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
    """

    def __init__(self):
        self.used_tool_name = None

    def on_tool_start(self, serialized: dict, input_str: str, **kwargs) -> None:
        print(f"üõ† on_tool_start: Tool: {serialized.get('name')}, Input: {input_str}")
        pass

    def on_tool_end(self, output: str, **kwargs) -> None:
        print("üõ† on_tool_end captured output (still debugging):", output)
        pass

    def on_agent_action(self, action: dict, **kwargs) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç)."""
        print(f"üïµÔ∏è‚Äç‚ôÄÔ∏è on_agent_action: Tool selected: {action.tool}, Tool Input: {action.tool_input}")
        self.used_tool_name = action.tool
        pass

    def on_agent_finish(self, finish: dict, **kwargs) -> None:
        print(f"üèÅ on_agent_finish: Agent finished. Output: {finish.return_values.get('output', 'N/A')}")
        pass


@tool
def add_asset_tool(text: str) -> str:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∫—É–ø–∫–µ –Ω–æ–≤–æ–≥–æ –∞–∫—Ç–∏–≤–∞ –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
    –û–∂–∏–¥–∞–µ—Ç —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–Ø –∫—É–ø–∏–ª –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É 0.00132 –±–∏—Ç–∫–æ–∏–Ω–∞ 22 –º–∞—Ä—Ç–∞ 2021 –≥–æ–¥–∞ –ø–æ —Ü–µ–Ω–µ 18000 –¥–æ–ª–ª–∞—Ä–æ–≤ –∑–∞ —à—Ç—É–∫—É", "–¥–æ–±–∞–≤—å 3 –∞–∫—Ü–∏–∏ Apple", "20 –º–∞—è –ø—Ä–∏–æ–±—Ä–µ–ª –¥–æ–ª–ª–∞—Ä –ø–æ 89 —Ä—É–±–ª–µ–π".
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º, —Ç–∏–∫–µ—Ä–æ–º, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º, —Ü–µ–Ω–æ–π –∏ –¥–∞—Ç–æ–π.
    """
    print("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞ –≤ –ë–î")
    asset_data = llm_insert_active(text, llm)

    result_summary = "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∫—É–ø–∫–µ –∞–∫—Ç–∏–≤–∞."
    if isinstance(asset_data, dict):
        name = asset_data.get('name_active', '–∞–∫—Ç–∏–≤–∞')
        count = asset_data.get('count')
        price = asset_data.get('price')
        currency = asset_data.get('currency', '')
        shortname = asset_data.get('shortname_active', 'N/A')

        parts = []
        if count is not None:
            parts.append(f"{count} –∞–∫—Ü–∏–π")
        if name:
            parts.append(name)
        if price is not None and currency:
            parts.append(f"–ø–æ —Ü–µ–Ω–µ {price} {currency}")
        elif price is not None:
            parts.append(f"–ø–æ —Ü–µ–Ω–µ {price}")

        if parts:
            result_summary = f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∫—É–ø–∫–µ {' '.join(parts)} –¥–æ–±–∞–≤–ª–µ–Ω–∞. –¢–∏–∫–µ—Ä: {shortname}."
        else:
            result_summary = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∫—É–ø–∫–µ –∞–∫—Ç–∏–≤–∞ –ø–æ–ª—É—á–µ–Ω–∞, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ–µ —Ä–µ–∑—é–º–µ."

    return json.dumps({
        "tool_name": "add_asset_tool",
        "result_text": result_summary,
        "data": asset_data
    }, ensure_ascii=False)


@tool
def analyze_news_tool(text: str) -> str:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∞–∫—Ç–∏–≤–∞–º.
    –û–∂–∏–¥–∞–µ—Ç —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –±–∏—Ç–∫–æ–∏–Ω–æ–º?", "–ö–∞–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ AMD?", "–ß—Ç–æ –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º —É —Å–±–µ—Ä–±–∞–Ω–∫–∞?".
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å—Ç—Ä–æ–≥—É—é —Å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–≤–æ–¥–æ–º –∏ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫. –ú–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å —Å–ª—É—á–∞–µ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ.
    """
    asset = llm_find_active(text, llm)
    if not asset or not asset.get("name_active"):
        return json.dumps({
            "tool_name": "analyze_news_tool",
            "result_text": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∞–∫—Ç–∏–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.",
            "link": None
        }, ensure_ascii=False)

    name_active = asset["name_active"]
    shortname_active = asset["shortname_active"]
    type_active = asset["type_active"]
    print(f"–ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è: {name_active}")

    result_text, link = llm_news_analysis(name_active, shortname_active, type_active, llm)

    if not result_text:
        return json.dumps({
            "tool_name": "analyze_news_tool",
            "result_text": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º.",
            "link": None
        }, ensure_ascii=False)

    return json.dumps({
        "tool_name": "analyze_news_tool",
        "result_text": result_text,
        "link": link
    }, ensure_ascii=False)


def create_sql_query_tool(user_id: int):
    @tool
    def sql_query_tool(text: str) -> str:
        """
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∞–∫—Ç–∏–≤–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –µ–≥–æ –∫–æ—à–µ–ª—å–∫–∞ (—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö).
        –û–∂–∏–¥–∞–µ—Ç —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–ö–∞–∫–∏–µ –∞–∫—Ç–∏–≤—ã —è –∫—É–ø–∏–ª –≤ –º–∞–µ 2025?", "–°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ —è –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª –≤ —Ä—É–±–ª—è—Ö?", "–ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —è –ø–æ–∫—É–ø–∞–ª –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É?"
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å—Ç—Ä–æ–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–ø—Ä–æ—Å–∞.
        """
        print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ë–î –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        response_data = llm_sql_database_toolkit(text, user_id, llm)
        return json.dumps({
            "tool_name": "sql_query_tool",
            "result_text": str(response_data)
        }, ensure_ascii=False)

    return sql_query_tool


@tool
def rag_tool(text: str) -> str:
    """
    –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ò–ï –∏–ª–∏ –û–ë–©–ò–ï –í–û–ü–†–û–°–´ –æ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö, —Ç–µ—Ä–º–∏–Ω–∞—Ö –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ RAG –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
    –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏/–ø—Ä–æ–¥–∞–∂–∏ –∏–ª–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∞–∫—Ç–∏–≤–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –û–∂–∏–¥–∞–µ—Ç —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞: "–ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å —Ä–∏—Å–∫–∏ –ø—Ä–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è—Ö?", "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ö—ç–¥–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–≤–æ–ø–∞—Ö?", "–ü–æ—á–µ–º—É –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö?"
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å—Ç—Ä–æ–∫—É —Å –æ—Ç–≤–µ—Ç–æ–º –∏ —Å–ø–∏—Å–∫–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    """
    print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ RAG")
    result_text = llm_with_RAG(text, llm)

    if not result_text:
        return json.dumps({
            "tool_name": "rag_tool",
            "result_text": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."
        }, ensure_ascii=False)

    return json.dumps({
        "tool_name": "rag_tool",
        "result_text": result_text
    }, ensure_ascii=False)


def llm_router(text: str, user_id: int)->dict:
    current_sql_query_tool = create_sql_query_tool(user_id)

    # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è used_tool_name –∏–∑ on_agent_action
    callback_handler = ToolUsageCallbackHandler()
    cb_manager = CallbackManager([callback_handler])

    agent = initialize_agent(
        tools=[add_asset_tool, analyze_news_tool, current_sql_query_tool, rag_tool],
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        verbose=True,
        callbacks=cb_manager,
        return_intermediate_steps=True
    )

    result_agent = agent.invoke({"input": text})

    used_tool = callback_handler.used_tool_name

    final_output_text = result_agent.get('output', "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç.")
    tool_data = {}

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π Observation –∏–∑ intermediate_steps
    intermediate_steps = result_agent.get('intermediate_steps', [])
    last_observation_str = None
    if intermediate_steps:
        _, last_observation_str = intermediate_steps[-1]

    if last_observation_str:
        try:
            tool_data = json.loads(last_observation_str)
        except json.JSONDecodeError:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ Observation: {last_observation_str}")
            tool_data = {"error": "Failed to parse tool output as JSON from intermediate steps"}
        except Exception as e:
            print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ intermediate steps: {e}")
            tool_data = {"error": f"An unexpected error occurred: {e}"}

    print(f"\n‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω —Å –ø–æ–º–æ—â—å—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {used_tool}")
    return {
        "output": final_output_text,
        "used_tool": used_tool,
        "tool_data": tool_data
    }